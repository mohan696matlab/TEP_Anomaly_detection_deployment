{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, File, UploadFile\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "\n",
    "# Later, in another script or session, you can load the scaler\n",
    "loaded_scaler = joblib.load('standard_scaler.pkl')\n",
    "# Load the saved autoencoder model\n",
    "loaded_autoencoder = load_model(\"autoencoder_model.h5\")\n",
    "# Load Featurewise Thresold\n",
    "feature_wise_threshold = joblib.load(\"feature_wise_thresold.pkl\")\n",
    "# Thresold for reconstruction loss\n",
    "threshold = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\TEP_deployment\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(r'Dataset/Fault_1.csv')\n",
    "\n",
    "x  = test_df.iloc[10,3:].values.tolist()\n",
    "x = loaded_scaler.transform([x])\n",
    "x_recon = loaded_autoencoder.predict(x)\n",
    "\n",
    "reconstruction_loss = np.mean((x-x_recon)**2,axis=1)/ threshold\n",
    "feature_wise_recon = np.abs(x-x_recon)/feature_wise_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FastAPI instance\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data model using Pydantic\n",
    "class InputData(BaseModel):\n",
    "    features: list[float]\n",
    "\n",
    "# Define a predict endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: InputData):\n",
    "    # Access the input data\n",
    "    input_values = data.features\n",
    "\n",
    "    # Transform the input data using the loaded scaler\n",
    "    x = loaded_scaler.transform([input_values])\n",
    "\n",
    "    # Predict using the loaded autoencoder\n",
    "    x_recon = loaded_autoencoder.predict(x)\n",
    "\n",
    "    # Calculate reconstruction loss\n",
    "    reconstruction_loss = np.mean((x - x_recon) ** 2, axis=1) / threshold\n",
    "\n",
    "    # Calculate feature-wise reconstruction\n",
    "    feature_wise_recon = np.abs(x - x_recon) / feature_wise_threshold\n",
    "\n",
    "    # Return the results\n",
    "    return {\"reconstruction_loss\": reconstruction_loss.tolist(), \"feature_wise_recon\": feature_wise_recon[0].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "{'reconstruction_loss': [0.1591294144058924], 'feature_wise_recon': [0.014048488366402665, 0.07167946043628737, 0.011968706024474815, 0.0015770186244844732, 0.029283588173224417, 0.006355635165405297, 0.11225035999592765, 0.033926212173757554, 0.01419359483124265, 0.026797744614696663, 0.04238606064262542, 0.09961963263940715, 0.031380712923322764, 0.034281741101467596, 0.04925037944707725, 0.15324045735262895, 0.031007532593777747, 0.152770774787654, 0.06025367743126983, 0.030583365375436055, 0.056213125667079464, 0.03034458768844238, 0.019222428499781712, 0.027514890704489765, 0.06856704697278396, 0.01809739019157608, 0.01873914649719986, 0.014364214013638346, 0.04835392928811603, 0.045712765830323465, 0.012217279599609514, 0.05724062558367255, 0.05670062964287344, 0.04569547025749758, 0.044976663065973116, 0.01367978819346021, 0.002169723375035531, 0.0597417429964809, 0.004326707888322666, 0.01528089147194902, 0.05934321568835971, 0.021892190315396336, 0.003666172121803143, 0.01479677790042369, 0.024559252524819986, 0.04299082031384656, 0.02111559421827347, 0.07044147960644694, 0.04598185804055445, 0.007301178459209844, 0.023843859685554046, 0.02256609489220791]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\TEP_deployment\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "# Test your endpoint using the TestClient\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "\n",
    "x_test  = test_df.iloc[10,3:].values.tolist()\n",
    "input_data = {\"features\": x_test}  # Provide the actual list of features\n",
    "\n",
    "# Send a POST request to the /predict endpoint\n",
    "response = client.post(\"/predict\", json=input_data)\n",
    "\n",
    "\n",
    "\n",
    "# Print the response JSON\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEP_deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
